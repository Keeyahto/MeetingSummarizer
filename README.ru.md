# Meeting Summarizer — демо возможностей для заказчика


https://github.com/user-attachments/assets/64b29ef9-46a0-4044-abec-b49617797de7


Этот репозиторий показывает пример построения прикладного ИИ‑сервиса «под задачу»: от загрузки аудио и распознавания речи до генерации структурированного резюме встречи, экспорта артефактов и простого веб‑интерфейса. Это именно демо, а не «готовая к деплою система»: цель — продемонстрировать продуманную архитектуру, качество реализации и понимание практических требований.

## Коротко о демо

- Принимает аудио (mp3/m4a/aac/wav/ogg/opus/webm), нормализует в WAV 16kHz.
- Распознает речь WhisperX, выравнивает слова по таймкоду и (опционально) присваивает спикеров.
- Генерирует итоги встречи через OpenAI‑совместимый LLM API: TL;DR, Action Items (с ответственными и сроками), Decisions, Risks — строго в JSON‑схеме.
- Экспортирует протокол (.md), совмещенный .json, субтитры .srt и .vtt.
- Отдает TL;DR потоково (SSE) — UI может показывать «живой» набор токенов.
- Считает метрики разговора: темп речи, доли говорящего времени, количество пауз, ключевые слова.

## Где это применимо

- Бизнес‑встречи и проектные созвоны: фиксация решений, назначение задач, контроль рисков.
- Продажи и поддержка: пост‑анализ звонков, унифицированные конспекты для CRM/вики.
- Исследовательские интервью и контент: быстрые выжимки и субтитры, таймлайны.
- Обучающие материалы и подкасты: markdown‑протоколы, навигация по темам и субтитры.

## Что реализовано (функционально)

- Распознавание и выравнивание слов: WhisperX с формированием «словных» субтитров.
- Диаризация: псевдодиаризация по паузам из коробки; при наличии HF token — pyannote.
- Резюме встречи: TL;DR + Action Items/Decisions/Risks в строгой JSON‑схеме; есть итеративный вариант объединения чанков.
- Экспорт: minutes.md, minutes.json (стенограмма+резюме), .srt, .vtt.
- Метрики и ключевые слова: базовые количественные индикаторы качества встречи.
- SSE стрим: потоковая выдача TL;DR для «живого» UX.

## Архитектура и стек

- Backend: FastAPI (`apps/api`), единая обработка ошибок, CORS, понятные эндпоинты.
- Обработка: `kits/kit_pipeline/pipeline.py` orchestration → ASR → метрики → LLM резюме → экспорт.
- ASR: `kits/kit_asr/whisperx_asr.py` (FFmpeg нормализация, WhisperX, выравнивание, диаризация).
- LLM: `kits/kit_llm/openai_backend.py` — совместимость с OpenAI API, json_schema, стриминг TL;DR; утилиты чанкинга и подсчета токенов.
- Экспорт: `kits/kit_export` — генерация SRT/VTT (по словам) и Markdown протокола.
- Общее: `kits/kit_common` — конфигурация (.env), пути, сериализация, ошибки.
- Async: Redis + RQ воркер (`apps/worker`), опционально; sync‑режим для упрощенного сценария.
- Frontend: Next.js 14 (`apps/web`), Tailwind, Zustand: загрузка, прогресс, вкладки результатов, кнопки экспорта, SSE TL;DR.
- Docker Compose: сервисы api/worker/redis, монтирование `data` и `models`.

## Интерфейс веб‑демо (UX)

- Загрузка файла: drag’n’drop/выбор, валидация формата и размера.
- Прогресс задачи: опрос статуса/переключение в результат.
- Вкладки результата: Overview (TL;DR, действия, решения, риски), Transcript (поиск, фильтр по спикерам), Subtitles, Topics.
- Загрузки: .md, .json, .srt, .vtt. Потоковый TL;DR (кнопка SSE) с плавным набором токенов.

## Что может быть интересно заказчику

- Адаптивность: легко изменить схему итогов (например, добавить «блокеры», «зависимости», «стоимость»), настроить подсветку доменных терминов, внедрить тонкую политику чанкинга.
- Локальность и приватность: работа с локальным LLM/ASR, гибкая маршрутизация запросов, отсутствие облачной передачи данных при желании.
- Масштабирование: переключение sync/async, очереди, воркеры, контейнеризация.
- Качество и тесты: модульные тесты для экспорта/пайплайна/API с моками тяжелых зависимостей; аккуратные ошибки и логирование.
- Переиспользуемость: модульная структура, четкие границы между ASR/LLM/экспортом.

## Эндпоинты (для понимания интеграции)

- `GET /health` — статус окружения.
- `POST /transcribe` — загрузка аудио; возвращает `job_id`.
- `GET /status/{job_id}` — статус обработки.
- `GET /result/{job_id}` — итог: стенограмма, резюме, метрики.
- `POST /summary/stream` — SSE‑поток TL;DR.
- `GET /export/{job_id}.(md|json|srt|vtt)` — выгрузка артефактов.
- `DELETE /result/{job_id}` — удаление результата.
- `GET /metrics` — агрегированные метрики по накопленным задачам.

## Ограничения демо

- Это прототип: не затрагиваются вопросы SSO, ролей/прав, продвинутого мониторинга/observability, биллинга и пр.
- Диаризация с pyannote зависит от HF token и GPU — в демо включена как опция.
- Качество итогов LLM зависит от выбранной модели и параметров (температура, бюджет токенов);
  предусмотрены утилиты для итеративной сводки и «бюджетирования», но подбор — предмет проекта.

## Технологии (основные)

- Python, FastAPI, Redis/RQ, PyTorch, WhisperX, FFmpeg.
- Next.js 14, TypeScript, Tailwind, Zustand.
- Тесты: pytest, Vitest. Docker Compose для локального окружения.

## Тесты и качество

- Pytest покрывает экспорт субтитров и markdown‑протокола, целостность пайплайна (с моками ASR/LLM), API (health/transcribe/result/exports/SSE).
- Тяжелые зависимости (ffmpeg, WhisperX, LLM‑запросы) замоканы для скорости и детерминизма.

## Чем я могу помочь в вашем проекте

- Подбор и интеграция моделей под домен (языки, термины, приватность, бюджет).
- Настройка качества итогов: промпт‑инжиниринг, пост‑обработка, схемы JSON, валидация.
- Интеграция с CRM/таск‑менеджерами/вики, подключение SSO и ролей.
- Продакшн‑окружение: контейнеризация, очереди, мониторинг, алерты.
- UI/UX доработка под сценарии (от «оператора кол‑центра» до «аналитика»).

Если задача близка к вашему кейсу — расскажите, какой стек и ограничения у вас уже есть (GPU/Cloud, требования к приватности, желаемые форматы и интеграции). Предложу реалистичный план, оценку и быстрый PoC.

